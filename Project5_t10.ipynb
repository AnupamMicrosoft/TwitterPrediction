{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10 Before Superbowl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_y(df,window):\n",
    "    num_features = 5\n",
    "    \n",
    "    tweets_by_hour = df.set_index('TimeStamp').groupby(pd.Grouper(freq='60Min'))\n",
    "    \n",
    "    total_hours = len(tweets_by_hour)\n",
    "\n",
    "    X = np.zeros((total_hours, 5))\n",
    "    for i, (key, val) in enumerate(tweets_by_hour):\n",
    "            features = [len(val), val.nReTweet.sum(), val.nFollowers.sum(), val.nFollowers.max(), key.hour]\n",
    "            X[i, :] = features\n",
    "            \n",
    "    y = X[:, 0][window:]\n",
    "    # number of tweets is the output as well as the first feature - but have to shift by #window hours\n",
    "    X = np.nan_to_num(X)\n",
    "    X_window = np.zeros((total_hours - window, num_features * window))\n",
    "    \n",
    "    for i in range(total_hours - window):\n",
    "        X_window[i, :] = np.concatenate([X[i+k, :] for k in range(window)])\n",
    "        \n",
    "    X_window = np.nan_to_num(X_window)\n",
    "    return X_window, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "piece_time = datetime.datetime(2015, 2, 1, 8, 0).timestamp()\n",
    "piece_time_pst = datetime.datetime.fromtimestamp(piece_time, pst_tz)\n",
    "piece_time_2 = datetime.datetime(2015, 2, 1, 20, 0).timestamp()\n",
    "piece_time_2_pst = datetime.datetime.fromtimestamp(piece_time_2, pst_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_to_df(tweet_d):\n",
    "    df = pd.DataFrame(tweet_d, columns=['ID', 'Title', 'nFollowers', 'nReTweet','Ranking Score','impressions','TimeStamp'])\n",
    "    df.sort_values(by='TimeStamp',inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tweets_into_df(file):\n",
    "    d = []\n",
    "    for line in open(file):\n",
    "        temp = []\n",
    "        t = json.loads(line)\n",
    "        time = datetime.datetime.fromtimestamp(t['citation_date'], pst_tz)\n",
    "        if(time < piece_time_pst ):\n",
    "            temp.append(t['tweet']['id'])\n",
    "            temp.append(t['title'])\n",
    "            #temp.append(t['user'])\n",
    "            temp.append(t['author']['followers'])\n",
    "            temp.append(t['metrics']['citations']['total'])\n",
    "            temp.append(t['metrics']['ranking_score'])\n",
    "            temp.append(t['metrics']['impressions'])\n",
    "            temp.append(datetime.datetime.fromtimestamp(t['citation_date'], pst_tz))\n",
    "            d.append(temp)\n",
    "    return tweet_to_df(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfl_df = extract_tweets_into_df(\"data/tweets_#nfl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb_df = extract_tweets_into_df(\"data/tweets_#superbowl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb49_df = extract_tweets_into_df(\"data/tweets_#sb49.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pat_df = extract_tweets_into_df(\"data/tweets_#patriots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hawks_df = extract_tweets_into_df(\"data/tweets_#gohawks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "go_pat_df = extract_tweets_into_df(\"data/tweets_#gopatriots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = [nfl_df,sb_df,sb49_df,pat_df,hawks_df,go_pat_df]\n",
    "#aggregated df\n",
    "df = pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-14 00:00:04-08:00\n",
      "2015-02-01 07:59:59-08:00\n",
      "440 hours\n"
     ]
    }
   ],
   "source": [
    "min_time = df.set_index('TimeStamp').index.min()\n",
    "max_time = df.set_index('TimeStamp').index.max()\n",
    "print (min_time)\n",
    "print (max_time)\n",
    "\n",
    "total_hours = (max_time - min_time)\n",
    "total_hours = total_hours.to_timedelta64().astype('timedelta64[h]') + 1\n",
    "print(total_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y = get_X_y(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "param_grid = {\"n_estimators\": [200, 400, 600, 800, 1000,1200, 1400, 1600, 1800, 2000],\n",
    "    \"max_depth\": [10, 20, 40, 60, 80, 100, 200, None],\n",
    "    \"max_features\": ['auto', 'sqrt'],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=0,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=KFold(5, shuffle=True),scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4428349.08819\n",
      "{'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75166805142689175"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBregressor = GradientBoostingRegressor(random_state=0, n_estimators=200, max_depth=10, max_features='auto', \n",
    "                                  min_samples_leaf=10, min_samples_split=2)\n",
    "GBregressor.fit(X_train, y_train)\n",
    "GBregressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2090700.0147\n"
     ]
    }
   ],
   "source": [
    "y_pred = GBregressor.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated Data - During Super Bowl (5 mins window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tweets_into_df(file):\n",
    "    d = []\n",
    "    for line in open(file):\n",
    "        temp = []\n",
    "        t = json.loads(line)\n",
    "        time = datetime.datetime.fromtimestamp(t['citation_date'], pst_tz)\n",
    "        if(time > piece_time_pst and time < piece_time_2_pst):\n",
    "            temp.append(t['tweet']['id'])\n",
    "            temp.append(t['title'])\n",
    "            #temp.append(t['user'])\n",
    "            temp.append(t['author']['followers'])\n",
    "            temp.append(t['metrics']['citations']['total'])\n",
    "            temp.append(t['metrics']['ranking_score'])\n",
    "            temp.append(t['metrics']['impressions'])\n",
    "            temp.append(datetime.datetime.fromtimestamp(t['citation_date'], pst_tz))\n",
    "            d.append(temp)\n",
    "    return tweet_to_df(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfl_df = extract_tweets_into_df(\"data/tweets_#nfl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb_df = extract_tweets_into_df(\"data/tweets_#superbowl.txt\")\n",
    "sb49_df = extract_tweets_into_df(\"data/tweets_#sb49.txt\")\n",
    "pat_df = extract_tweets_into_df(\"data/tweets_#patriots.txt\")\n",
    "hawks_df = extract_tweets_into_df(\"data/tweets_#gohawks.txt\")\n",
    "go_pat_df = extract_tweets_into_df(\"data/tweets_#gopatriots.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-02-01 08:00:01-08:00\n",
      "2015-02-01 19:59:59-08:00\n",
      "12 hours\n"
     ]
    }
   ],
   "source": [
    "frame = [nfl_df,sb_df,sb49_df,pat_df,hawks_df,go_pat_df]\n",
    "#aggregated df\n",
    "df = pd.concat(frame)\n",
    "\n",
    "min_time = df.set_index('TimeStamp').index.min()\n",
    "max_time = df.set_index('TimeStamp').index.max()\n",
    "print (min_time)\n",
    "print (max_time)\n",
    "\n",
    "total_hours = (max_time - min_time)\n",
    "total_hours = total_hours.to_timedelta64().astype('timedelta64[h]') + 1\n",
    "print(total_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y = get_X_y(df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=0,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=KFold(5, shuffle=True),scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7868149821.4\n",
      "{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.93462157095302656"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBregressor = GradientBoostingRegressor(random_state=0, n_estimators=1000, max_depth=10, max_features='sqrt', \n",
    "                                  min_samples_leaf=2, min_samples_split=2)\n",
    "GBregressor.fit(X_train, y_train)\n",
    "GBregressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 8613113559.5608\n"
     ]
    }
   ],
   "source": [
    "y_pred = GBregressor.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Super Bowl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tweets_into_df(file):\n",
    "    d = []\n",
    "    for line in open(file):\n",
    "        temp = []\n",
    "        t = json.loads(line)\n",
    "        time = datetime.datetime.fromtimestamp(t['citation_date'], pst_tz)\n",
    "        if(time > piece_time_2_pst):\n",
    "            temp.append(t['tweet']['id'])\n",
    "            temp.append(t['title'])\n",
    "            #temp.append(t['user'])\n",
    "            temp.append(t['author']['followers'])\n",
    "            temp.append(t['metrics']['citations']['total'])\n",
    "            temp.append(t['metrics']['ranking_score'])\n",
    "            temp.append(t['metrics']['impressions'])\n",
    "            temp.append(datetime.datetime.fromtimestamp(t['citation_date'], pst_tz))\n",
    "            d.append(temp)\n",
    "    return tweet_to_df(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfl_df = extract_tweets_into_df(\"data/tweets_#nfl.txt\")\n",
    "sb_df = extract_tweets_into_df(\"data/tweets_#superbowl.txt\")\n",
    "sb49_df = extract_tweets_into_df(\"data/tweets_#sb49.txt\")\n",
    "pat_df = extract_tweets_into_df(\"data/tweets_#patriots.txt\")\n",
    "hawks_df = extract_tweets_into_df(\"data/tweets_#gohawks.txt\")\n",
    "go_pat_df = extract_tweets_into_df(\"data/tweets_#gopatriots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-02-01 20:00:01-08:00\n",
      "2015-02-07 10:55:36-08:00\n",
      "135 hours\n"
     ]
    }
   ],
   "source": [
    "frame = [nfl_df,sb_df,sb49_df,pat_df,hawks_df,go_pat_df]\n",
    "#aggregated df\n",
    "df = pd.concat(frame)\n",
    "\n",
    "min_time = df.set_index('TimeStamp').index.min()\n",
    "max_time = df.set_index('TimeStamp').index.max()\n",
    "print (min_time)\n",
    "print (max_time)\n",
    "\n",
    "total_hours = (max_time - min_time)\n",
    "total_hours = total_hours.to_timedelta64().astype('timedelta64[h]') + 1\n",
    "print(total_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=0,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_depth': [10, 20, 40, 60, 80, 100, 200, None], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [2, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = get_X_y(df, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=KFold(5, shuffle=True),scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-394456.209\n",
      "{'max_depth': 20, 'max_features': 'auto', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92813335568695532"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBregressor = GradientBoostingRegressor(random_state=0, n_estimators=200, max_depth=20, max_features='auto', \n",
    "                                  min_samples_leaf=5, min_samples_split=2)\n",
    "GBregressor.fit(X_train, y_train)\n",
    "GBregressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 211126.2024\n"
     ]
    }
   ],
   "source": [
    "y_pred = GBregressor.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE: %.4f\" % mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
