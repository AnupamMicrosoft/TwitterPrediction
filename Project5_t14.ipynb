{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "path = \"./tweet_data\" \n",
    "files= os.listdir(path) \n",
    "print(files)\n",
    "all_tweets = {}\n",
    "for file in files: \n",
    "     if not os.path.isdir(file): \n",
    "        f = open(path + \"/\" + file) \n",
    "        templist = []\n",
    "        key = file[7:-4]\n",
    "        for line in open(path + \"/\" + file): \n",
    "            line = f.readline()\n",
    "            tweet = json.loads(line)\n",
    "            templist.append(tweet)\n",
    "        all_tweets.setdefault(key, list)\n",
    "        all_tweets[key] = templist\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction in each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "import pytz\n",
    "\n",
    "def feature_ext_period(start_time, end_time, tweetfile): \n",
    "    # only consider a part of the data, not every tweets in the file\n",
    "    total_hours = (end_time - start_time) / 3600 + 1\n",
    "    # print(total_hours)\n",
    "    tweets_totalnum_perhour = [0 for i in range(total_hours)]\n",
    "    retweets_num_perhour = [0 for i in range(total_hours)]\n",
    "    followers_totalnum_perhour = [0 for i in range(total_hours)]\n",
    "    followers_maxnum_perhour = [0 for i in range(total_hours)]\n",
    "    time_of_day_perhour = [None for i in range(total_hours)]\n",
    "    timestamp_perhour = []\n",
    "    ts = start_time\n",
    "    for i in range(total_hours):\n",
    "        timestamp_perhour.append(ts)\n",
    "        ts += 3600\n",
    "    pst_tz = pytz.timezone('US/Pacific') \n",
    "    \n",
    "    for tweet in tweetfile: \n",
    "        tweet_time = tweet['citation_date']\n",
    "        if (tweet_time >= start_time and tweet_time < end_time + 3600):\n",
    "            #hour = int((tweet_time - po_time_min)/3600)\n",
    "            hour = int((tweet_time - start_time) / 3600)\n",
    "            tweets_totalnum_perhour[hour] += 1\n",
    "            retweets_num_perhour[hour] += tweet['tweet']['retweet_count']\n",
    "            followers_totalnum_perhour[hour] += tweet['author']['followers']\n",
    "            if (tweet['author']['followers'] >= followers_maxnum_perhour[hour]):\n",
    "                followers_maxnum_perhour[hour] = tweet['author']['followers']\n",
    "            if (time_of_day_perhour[hour] == None):\n",
    "                earliest_date = datetime.datetime.fromtimestamp(tweet['citation_date'], pst_tz)\n",
    "                time_of_day_perhour[hour] = earliest_date.hour\n",
    "    feat_5 = [tweets_totalnum_perhour[0:-1], retweets_num_perhour[0:-1], followers_totalnum_perhour[0:-1],\\\n",
    "              followers_maxnum_perhour[0:-1],time_of_day_perhour[0:-1]]\n",
    "    gt_y = tweets_totalnum_perhour[1:]\n",
    "    return timestamp_perhour[0:-1], feat_5, gt_y\n",
    "    # return index_timestamp, X_features, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(start_time, end_time, tweetfile): \n",
    "\n",
    "    total_hours = (end_time - start_time) / 3600 + 1\n",
    "    # other features:\n",
    "    ori_author_followers = [0 for i in range(total_hours)] \n",
    "    favorited_num = [0 for i in range(total_hours)] \n",
    "    impressions_num = [0 for i in range(total_hours)]\n",
    "    avg_ranking_score = [0 for i in range(total_hours)]\n",
    "    user_mentions = [0 for i in range(total_hours)]\n",
    "    url_count = [0 for i in range(total_hours)]\n",
    "    unique_author_set = [set() for i in range(total_hours)] \n",
    "    \n",
    "    # extract basic features\n",
    "    index, feat_5, gt_y = extract_features(start_time, end_time, tweetfile)\n",
    "\n",
    "    for tweet in tweetfile:\n",
    "        tweet_time = tweet['citation_date']\n",
    "        if (tweet_time >= start_time and tweet_time < end_time +3600):\n",
    "            hour = int((tweet_time - start_time) / 3600)\n",
    "            ori_author_followers[hour] += tweet['original_author']['followers']\n",
    "            favorited_num[hour] += tweet['tweet']['favorite_count']\n",
    "            user_mentions[hour] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "            url_count[hour] += len(tweet['tweet']['entities']['urls'])\n",
    "            unique_author_set[hour].add(tweet['author']['nick'])\n",
    "            impressions_num[hour] += tweet['metrics']['impressions']\n",
    "            avg_ranking_score[hour] += tweet['metrics']['ranking_score'] \n",
    "            \n",
    "    total_tweets = feat_5[0]\n",
    "    for i in range(0, len(total_tweets)):\n",
    "        if(total_tweets[i] != 0):\n",
    "            avg_ranking_score[i] = avg_ranking_score[i] / total_tweets[i]\n",
    "    \n",
    "    unique_author_count =  [len(val) for val in unique_author_set]  \n",
    "    feat_extra = [ori_author_followers[0:-1], favorited_num[0:-1], \\\n",
    "                  user_mentions[0:-1], url_count[0:-1], unique_author_count[0:-1],\\\n",
    "                  impressions_num[0:-1],avg_ranking_score[0:-1]]\n",
    "\n",
    "    feat_all = feat_5 + feat_extra\n",
    "\n",
    "    return index, feat_all, gt_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_tz = pytz.timezone('US/Pacific') \n",
    "round_timestamp_to_hour = lambda t: int(time.mktime(datetime.datetime.fromtimestamp(t, pst_tz).replace(microsecond=0,second=0,minute=0).timetuple()))\n",
    "\n",
    "mins = {}\n",
    "maxs = {}\n",
    "for key in all_tweets:\n",
    "    tmp_min = all_tweets[key][0]['citation_date']\n",
    "    tmp_max = 0\n",
    "    for tweet in all_tweets[key]:\n",
    "        tmp_min = min(tmp_min, tweet['citation_date'])\n",
    "        tmp_max = max(tmp_max, tweet['citation_date'])\n",
    "    mins[key] = round_timestamp_to_hour(tmp_min)\n",
    "    maxs[key] = round_timestamp_to_hour(tmp_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def features_transform(x):\n",
    "    for i in range(len(x[4])):\n",
    "        if x[4][i] == None:\n",
    "            x[4][i] = (x[4][i - 1] + 1) % 24\n",
    "    X = np.array(x)\n",
    "    X = np.transpose(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "indexs, Xs, Ys = [], [], []\n",
    "for key in all_tweets:\n",
    "    index, X, y = extract_features(mins[key], maxs[key], all_tweets[key])\n",
    "    X = features_transform(X)\n",
    "    y = np.array(y)\n",
    "    indexs.append(index)\n",
    "    Xs.append(X)\n",
    "    Ys.append(y)\n",
    "\n",
    "pickle.dump(indexs, open(\"indexs.txt\", \"w\"))\n",
    "pickle.dump(Xs, open(\"Xs.txt\", \"w\"))\n",
    "pickle.dump(Ys, open(\"Ys.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = int(time.mktime(datetime.datetime(2015, 2, 1, 8, 0, 0, 0, pst_tz).timetuple()))\n",
    "time2 = int(time.mktime(datetime.datetime(2015, 2, 1, 20, 0, 0, 0, pst_tz).timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# regression models\n",
    "def regression_models(X, y):\n",
    "    XX = sm.add_constant(X)\n",
    "    kf = KFold(n_splits=10)\n",
    "    rmses = [0.0, 0.0, 0.0]\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        XX_train, XX_test = XX[train_index], XX[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # linear\n",
    "        model = sm.regression.linear_model.OLS(y_train, XX_train)\n",
    "        results = model.fit()\n",
    "        prediction = results.predict(XX_test)\n",
    "        rmses[0] += math.sqrt(np.mean((y_test - prediction) ** 2))\n",
    "        # KNN\n",
    "        knn = KNeighborsRegressor(n_neighbors=10, n_jobs=-1)\n",
    "        knn.fit(X_train, y_train)\n",
    "        prediction = knn.predict(X_test)\n",
    "        rmses[1] += math.sqrt(np.mean((y_test - prediction) ** 2))\n",
    "        # Random Forrest\n",
    "        regr = RandomForestRegressor(n_jobs=-1)\n",
    "        regr.fit(X_train, y_train)\n",
    "        prediction = regr.predict(X_test)\n",
    "        rmses[2] += math.sqrt(np.mean((y_test - prediction) ** 2))\n",
    "        \n",
    "    return [rmse / kf.get_n_splits() for rmse in rmses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = pickle.load(open(\"indexs.txt\", \"r\"))\n",
    "Xs = pickle.load(open(\"Xs.txt\", \"r\"))\n",
    "Ys = pickle.load(open(\"Ys.txt\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = {}\n",
    "for i in range(len(indexs)):\n",
    "    key = files[i][7:-4]\n",
    "    rmses[key] = []\n",
    "    idx1, idx2 = indexs[i].index(time1), indexs[i].index(time2)\n",
    "    rmses[key].append(regression_models(Xs[i][:idx1], Ys[i][:idx1]))\n",
    "    rmses[key].append(regression_models(Xs[i][idx1:idx2], Ys[i][idx1:idx2]))\n",
    "    rmses[key].append(regression_models(Xs[i][idx2:], Ys[i][idx2:]))\n",
    "\n",
    "print(rmses)\n",
    "pickle.dump(rmses, open(\"rmses.txt\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate through all tags\n",
    "X_train, y_train = [], []\n",
    "vector_X, vector_y = [[],[],[]], [[],[],[]]\n",
    "for i in range(len(indexs)):\n",
    "    idx1, idx2 = indexs[i].index(time1), indexs[i].index(time2)\n",
    "    vector_X[0].append(Xs[i][:idx1])\n",
    "    vector_y[0].append(Ys[i][:idx1])\n",
    "    vector_X[1].append(Xs[i][idx1:idx2])\n",
    "    vector_y[1].append(Ys[i][idx1:idx2])\n",
    "    vector_X[2].append(Xs[i][idx2:])\n",
    "    vector_y[2].append(Ys[i][idx2:])\n",
    "\n",
    "for i in range(3):\n",
    "    X_train.append(np.concatenate(vector_X[i]))\n",
    "    y_train.append(np.concatenate(vector_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_jobs=-1)\n",
    "rmses_all = []\n",
    "# period 1\n",
    "regr.fit(X_train[0], y_train[0])\n",
    "prediction = regr.predict(X_train[0])\n",
    "rmses_all.append(math.sqrt(np.mean((y_train[0] - prediction) ** 2)))\n",
    "# period 2\n",
    "regr.fit(X_train[1], y_train[1])\n",
    "prediction = regr.predict(X_train[1])\n",
    "rmses_all.append(math.sqrt(np.mean((y_train[1] - prediction) ** 2)))\n",
    "# period 3\n",
    "regr.fit(X_train[2], y_train[2])\n",
    "prediction = regr.predict(X_train[2])\n",
    "rmses_all.append(math.sqrt(np.mean((y_train[2] - prediction) ** 2)))\n",
    "print(rmses_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data to 6 hour time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectors_X = []\n",
    "vectors_y = []    \n",
    "for i in range(len(Xs)):\n",
    "    for j in range(len(Xs[i]) - 5):\n",
    "        vectors_X.append(np.concatenate([Xs[i][j], Xs[i][j+1], Xs[i][j+2], Xs[i][j+3], Xs[i][j+4]]))\n",
    "    vectors_y.extend(Ys[i][5:])\n",
    "\n",
    "X_new = np.vstack(vectors_X)\n",
    "delete_index = [ 1, 2, 4, 6, 9,10,11,\n",
    "                13,14,16,18,21,22,23,\n",
    "                25,26,28,30,33,34,35,\n",
    "                37,38,40,42,45,46,47,\n",
    "                49,50,52,54,57,58,59]\n",
    "X_new = np.delete(X_new, delete_index, 1) \n",
    "y_new = np.array(vectors_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_jobs=-1)\n",
    "regr.fit(X_new, y_new)\n",
    "print(\"Feature vector length: %d\" % len(X_new[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_timestamp(tweetlist):\n",
    "    tmp_max = 0\n",
    "    for tweet in tweetlist:\n",
    "        tmp_max = max(tmp_max, tweet['citation_date'])\n",
    "\n",
    "    tmp_max = round_timestamp_to_hour(tmp_max)\n",
    "    return tmp_max - 3600*5, tmp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./test_data\" \n",
    "files= os.listdir(file_path)\n",
    "for file in files: \n",
    "     if not os.path.isdir(file):\n",
    "        print(\"File: %s\" % file)\n",
    "        tweetlist = []\n",
    "        with open(file_path + \"/\" + file) as f: \n",
    "            for line in f.readlines(): \n",
    "                tweet = json.loads(line)\n",
    "                tweetlist.append(tweet)\n",
    "\n",
    "        start_time, end_time = get_start_end_timestamp(tweetlist)\n",
    "        index, X, y = extract_features(start_time, end_time, tweetlist)\n",
    "\n",
    "        X[4][0] = datetime.datetime.fromtimestamp(start_time, pst_tz).hour\n",
    "        for i in range(1, len(X[4])):\n",
    "            X[4][i] = (X[4][i - 1] + 1) % 24\n",
    "\n",
    "        X = np.array(X)\n",
    "        X = np.transpose(X).astype('int')\n",
    "        X = np.delete(X, [1,2,4,6,9,10,11], 1)\n",
    "        y = np.array(y)\n",
    "\n",
    "        y_test = y[-1]\n",
    "        X_test = np.concatenate([X[0], X[1], X[2], X[3], X[4]])\n",
    "        X_test = np.vstack([X_test])\n",
    "\n",
    "        print(\"Truth: %d\" % y_test)\n",
    "        print(\"Predict: %d\" % regr.predict(X_test)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
