{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_X_y(df,window):\n",
    "    num_features = 5\n",
    "    \n",
    "    tweets_by_hour = df.set_index('TimeStamp').groupby(pd.Grouper(freq='60Min'))\n",
    "    \n",
    "    total_hours = len(tweets_by_hour)\n",
    "\n",
    "    X = np.zeros((total_hours, 5))\n",
    "    for i, (key, val) in enumerate(tweets_by_hour):\n",
    "            features = [len(val), val.nReTweet.sum(), val.nFollowers.sum(), val.nFollowers.max(), key.hour]\n",
    "            X[i, :] = features\n",
    "            \n",
    "    y = X[:, 0][window:]\n",
    "    # number of tweets is the output as well as the first feature - but have to shift by #window hours\n",
    "    X = np.nan_to_num(X)\n",
    "    X_window = np.zeros((total_hours - window, num_features * window))\n",
    "    \n",
    "    for i in range(total_hours - window):\n",
    "        X_window[i, :] = np.concatenate([X[i+k, :] for k in range(window)])\n",
    "        \n",
    "    X_window = np.nan_to_num(X_window)\n",
    "    return X_window, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "piece_time = datetime.datetime(2015, 2, 1, 8, 0).timestamp()\n",
    "piece_time_pst = datetime.datetime.fromtimestamp(piece_time, pst_tz)\n",
    "piece_time_2 = datetime.datetime(2015, 2, 1, 20, 0).timestamp()\n",
    "piece_time_2_pst = datetime.datetime.fromtimestamp(piece_time_2, pst_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_to_df(tweet_d):\n",
    "    df = pd.DataFrame(tweet_d, columns=['ID', 'Title', 'nFollowers', 'nReTweet','Ranking Score','impressions','TimeStamp'])\n",
    "    df.sort_values(by='TimeStamp',inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tweets_into_df(file):\n",
    "    d = []\n",
    "    for line in open(file):\n",
    "        temp = []\n",
    "        t = json.loads(line)\n",
    "        time = datetime.datetime.fromtimestamp(t['citation_date'], pst_tz)\n",
    "        #if(time < piece_time_pst ):\n",
    "        temp.append(t['tweet']['id'])\n",
    "        temp.append(t['title'])\n",
    "        #temp.append(t['user'])\n",
    "        temp.append(t['author']['followers'])\n",
    "        temp.append(t['metrics']['citations']['total'])\n",
    "        temp.append(t['metrics']['ranking_score'])\n",
    "        temp.append(t['metrics']['impressions'])\n",
    "        temp.append(datetime.datetime.fromtimestamp(t['citation_date'], pst_tz))\n",
    "        d.append(temp)\n",
    "    return tweet_to_df(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfl_df = extract_tweets_into_df(\"data/tweets_#nfl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb_df = extract_tweets_into_df(\"data/tweets_#superbowl.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sb49_df = extract_tweets_into_df(\"data/tweets_#sb49.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pat_df = extract_tweets_into_df(\"data/tweets_#patriots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hawks_df = extract_tweets_into_df(\"data/tweets_#gohawks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "go_pat_df = extract_tweets_into_df(\"data/tweets_#gopatriots.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = [nfl_df,sb_df,sb49_df,pat_df,hawks_df,go_pat_df]\n",
    "#aggregated df\n",
    "df = pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-14 00:00:04-08:00\n",
      "2015-02-07 10:55:36-08:00\n",
      "587 hours\n"
     ]
    }
   ],
   "source": [
    "min_time = df.set_index('TimeStamp').index.min()\n",
    "max_time = df.set_index('TimeStamp').index.max()\n",
    "print (min_time)\n",
    "print (max_time)\n",
    "\n",
    "total_hours = (max_time - min_time)\n",
    "total_hours = total_hours.to_timedelta64().astype('timedelta64[h]') + 1\n",
    "print(total_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y = get_X_y(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn  import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMLP(mlp, X_train, y_train,X_test, y_test):\n",
    "    mlp.fit(X_train, y_train)\n",
    "    mlp.score(X_test, y_test)\n",
    "    predictions = mlp.predict(X_test)\n",
    "\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "    #print(\"MSE: %.4f\" % mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaul MLP\n",
    "\n",
    "1 hidden layer with 100 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPRegressor()\n",
    "mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Value of arch #1 40501079369.6721\n"
     ]
    }
   ],
   "source": [
    "mse = computeMLP(mlp,X_train, y_train,X_test, y_test )\n",
    "print(\"MSE Value of arch #1 %.4f\"%mse) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Architecture\n",
    "(10,10,10) if you want 3 hidden layers with 10 hidden units each. ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(10,10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Value of arch #2 3879255768246.4858\n"
     ]
    }
   ],
   "source": [
    "mse = computeMLP(mlp,X_train, y_train,X_test, y_test )\n",
    "print(\"MSE Value of arch #2 %.4f\"%mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Architecture\n",
    "architecture 56:25:11:7:5:3:1 with input 56 and 1 output hidden layers will be (25:11:7:5:3). So tuple hidden_layer_sizes = (25,11,7,5,3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Value of arch #3 629122112.8263\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(25,11,7,5,3,))\n",
    "mse = computeMLP(mlp,X_train, y_train,X_test, y_test )\n",
    "print(\"MSE Value of arch #3 %.4f\"%mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Architecture\n",
    "architecture 3:45:2:11:2 with input 3 and 2 output hidden layers will be (45:2:11). So tuple hidden_layer_sizes = (45,2,11,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Value of arch #4 629108381.9237\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(45,2,11,))\n",
    "mse = computeMLP(mlp,X_train, y_train,X_test, y_test )\n",
    "print(\"MSE Value of arch #4 %.4f\"%mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th Architecture\n",
    "1 hidden layer with 7 hidden units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Value of arch #5 186745293.9044\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(30,30,30))\n",
    "mse = computeMLP(mlp,X_train, y_train,X_test, y_test )\n",
    "print(\"MSE Value of arch #5 %.4f\"%mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling of data on 5th Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Value of arch #5 154110962.9271\n"
     ]
    }
   ],
   "source": [
    "mse = computeMLP(mlp,X_train, y_train,X_test, y_test )\n",
    "print(\"MSE Value of arch #5 %.4f\"%mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 154110962.9271\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
